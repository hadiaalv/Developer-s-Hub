# -*- coding: utf-8 -*-
"""Fake News Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X6CqxPGcbe15jmgxQIb5mTXwnP1NkOdy

# **LOADING DATASET**
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile


zip_path = '/content/drive/MyDrive/fake-and-real-news-dataset/fake-and-real-news-dataset.zip'

# Extraction path
extract_to = '/content/fake_news_data/'

# Unzipping
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

import pandas as pd

# Read CSVs after extraction
true_df = pd.read_csv('/content/fake_news_data/True.csv')
fake_df = pd.read_csv('/content/fake_news_data/Fake.csv')

# Add labels
true_df['label'] = 1
fake_df['label'] = 0

# Combine datasets
df = pd.concat([true_df, fake_df], ignore_index=True)

# Preview
df.head()

"""# **NLP LIBRARIES**"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer


nltk.download('stopwords')
nltk.download('wordnet')

"""# **PREPROCESSING**"""

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = text.split()
    tokens = [w for w in tokens if w not in stop_words]
    tokens = [lemmatizer.lemmatize(stemmer.stem(w)) for w in tokens]
    return " ".join(tokens)

# Apply on the combined dataframe
df['text'] = df['text'].apply(preprocess)

"""# **TEXT TO FEATURES**"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['text'])
y = df['label']

"""# **TRAINING NAIVE BAYES MODEL**




"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = MultinomialNB()
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

import joblib

# Save locally
joblib.dump(model, '/content/fake_news_model.pkl')
joblib.dump(vectorizer, '/content/vectorizer.pkl')

# Copy to Drive
!cp /content/fake_news_model.pkl /content/drive/MyDrive/fake-and-real-news-dataset/
!cp /content/vectorizer.pkl /content/drive/MyDrive/fake-and-real-news-dataset/

def predict_news(text):
    text = preprocess(text)
    text_vector = vectorizer.transform([text])
    prediction = model.predict(text_vector)[0]
    return "Real News" if prediction == 1 else "Fake News"

# Example:
predict_news("The prime minister held a conference to discuss economic growth...")

"""# **WEB APP**"""

from google.colab import drive
drive.mount('/content/drive')

model_path = '/content/drive/My Drive/fake-and-real-news-dataset/fake_news_model.pkl'
model = joblib.load(model_path)

!ls

import joblib


model = joblib.load('fake_news_model.pkl')
vectorizer = joblib.load('vectorizer.pkl')

!pip install streamlit pyngrok

code = """
import streamlit as st
import joblib
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import string

nltk.download('stopwords')

# Load model + vectorizer
model      = joblib.load("/content/drive/MyDrive/fake-and-real-news-dataset/fake_news_model.pkl")
vectorizer = joblib.load("/content/drive/MyDrive/fake-and-real-news-dataset/vectorizer.pkl")

stemmer = PorterStemmer()
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\d+', '', text)
    tokens = text.split()
    tokens = [stemmer.stem(w) for w in tokens if w not in stopwords.words('english')]
    return ' '.join(tokens)

st.set_page_config(page_title="Fake News Detector", layout="centered")
st.title("Fake News Detection App")
news_input = st.text_area("Paste your news article here:", height=250)

if st.button("Check News"):
    if not news_input.strip():
        st.warning("Please enter a news article.")
    else:
        processed = preprocess_text(news_input)
        vect      = vectorizer.transform([processed])
        pred      = model.predict(vect)[0]
        if pred == 0:
            st.error("This news article is likely **Fake**.")
        else:
            st.success("This news article appears to be **Real**.")
"""
with open("streamlit_app.py", "w") as f:
    f.write(code)

!ngrok config add-authtoken 2vzIoU44gSiCWMfB58bGCz6Tdoj_5nHhSiPnpKhHccMNH4e8A

from pyngrok import ngrok
import os

# Kill any previous Streamlit or ngrok processes
os.system("pkill streamlit")

# Start the Streamlit app in the background
os.system("streamlit run streamlit_app.py &")

# Connect to localhost:8501 using ngrok
public_url = ngrok.connect("http://localhost:8501", bind_tls=True)
print("Streamlit App URL:", public_url)